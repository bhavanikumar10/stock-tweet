{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tweepy\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from config2 import consumer_key, consumer_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API Keys\n",
    "consumer_key = consumer_key\n",
    "consumer_secret = consumer_secret\n",
    "access_token = access_token\n",
    "access_token_secret = access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_term = '@NintendoAmerica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9886\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nin_tweets = []\n",
    "\n",
    "date = []\n",
    "\n",
    "tweet_ids = []\n",
    "\n",
    "oldest_tweet = None\n",
    "for x in range(1,100):\n",
    "    public_tweets = api.search(target_term, count=100, result_type=\"recent\", max_id=oldest_tweet)\n",
    "\n",
    "    for tweet in public_tweets['statuses']:\n",
    "        tweet_id = tweet[\"id\"]\n",
    "        tweet_author = tweet[\"user\"][\"screen_name\"]\n",
    "        tweet_text = tweet[\"text\"]\n",
    "\n",
    "        nin_tweets.append(tweet['text'])\n",
    "        date.append(tweet['created_at'])\n",
    "        tweet_ids.append(tweet['id'])\n",
    "        \n",
    "        oldest_tweet = tweet_id - 1\n",
    "\n",
    "print(len(nin_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9900\n"
     ]
    }
   ],
   "source": [
    "nin_tweets2 = []\n",
    "\n",
    "date2 = []\n",
    "\n",
    "tweet_ids2 = []\n",
    "\n",
    "oldest_tweet2 = tweet_ids[9885]\n",
    "for x in range(1,100):\n",
    "    public_tweets = api.search(target_term, count=100, result_type=\"recent\", max_id=oldest_tweet)\n",
    "\n",
    "    for tweet in public_tweets['statuses']:\n",
    "        tweet_id = tweet[\"id\"]\n",
    "        tweet_author = tweet[\"user\"][\"screen_name\"]\n",
    "        tweet_text = tweet[\"text\"]\n",
    "\n",
    "        nin_tweets2.append(tweet['text'])\n",
    "        date2.append(tweet['created_at'])\n",
    "        tweet_ids2.append(tweet['id'])\n",
    "        \n",
    "        oldest_tweet2 = tweet_id - 1\n",
    "\n",
    "print(len(nin_tweets2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_list = []\n",
    "positive_list = []\n",
    "negative_list = []\n",
    "neutral_list = []\n",
    "\n",
    "for tweet in nin_tweets:\n",
    "\n",
    "    # Run Vader Analysis on each tweet\n",
    "    results = analyzer.polarity_scores(tweet)\n",
    "    compound = results[\"compound\"]\n",
    "    pos = results[\"pos\"]\n",
    "    neu = results[\"neu\"]\n",
    "    neg = results[\"neg\"]\n",
    "\n",
    "    # Add each value to the appropriate list\n",
    "    compound_list.append(compound)\n",
    "    positive_list.append(pos)\n",
    "    negative_list.append(neg)\n",
    "    neutral_list.append(neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_list2 = []\n",
    "positive_list2 = []\n",
    "negative_list2 = []\n",
    "neutral_list2 = []\n",
    "\n",
    "for tweet in nin_tweets2:\n",
    "\n",
    "    # Run Vader Analysis on each tweet\n",
    "    results = analyzer.polarity_scores(tweet)\n",
    "    compound = results[\"compound\"]\n",
    "    pos = results[\"pos\"]\n",
    "    neu = results[\"neu\"]\n",
    "    neg = results[\"neg\"]\n",
    "\n",
    "    # Add each value to the appropriate list\n",
    "    compound_list2.append(compound)\n",
    "    positive_list2.append(pos)\n",
    "    negative_list2.append(neg)\n",
    "    neutral_list2.append(neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_18_N_1 = {\n",
    "    'Text': nin_tweets,\n",
    "    'Compounded': compound_list,\n",
    "    'Negative': negative_list,\n",
    "    'Positive': positive_list,\n",
    "    'Neutral': neutral_list,\n",
    "    'Date': date\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_18_N_2 = {\n",
    "    'Text': nin_tweets2,\n",
    "    'Compounded': compound_list2,\n",
    "    'Negative': negative_list2,\n",
    "    'Positive': positive_list2,\n",
    "    'Neutral': neutral_list2,\n",
    "    'Date': date2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "june_18_nin_df = pd.DataFrame(june_18_N_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_18_nin_2df = pd.DataFrame(june_18_N_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# june_17_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date[9831]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_timestamps = []\n",
    "for raw in date:\n",
    "    # https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "    # http://strftime.org/\n",
    "    converted_time = datetime.strptime(raw, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    converted_timestamps.append(converted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_timestamps2 = []\n",
    "for raw in date2:\n",
    "    # https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "    # http://strftime.org/\n",
    "    converted_time = datetime.strptime(raw, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    converted_timestamps2.append(converted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = []\n",
    "for x in range(len(converted_timestamps)):\n",
    "    hours = converted_timestamps[x].hour\n",
    "    hour.append(hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour2 = []\n",
    "for x in range(len(converted_timestamps2)):\n",
    "    hours = converted_timestamps2[x].hour\n",
    "    hour2.append(hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_18_nin_df['Hour'] = hour\n",
    "june_18_nin_2df['Hour'] = hour2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# june_17_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_18_nin_df.to_csv('june_18_N_1.csv')\n",
    "june_18_nin_2df.to_csv('june_18_N_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_18_mean_nin_df = june_18_nin_df.groupby('Hour').mean()\n",
    "june_18_mean_nin_2df = june_18_nin_2df.groupby('Hour').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_18_mean_nin_df.to_csv('june_18_mean_N_1.csv')\n",
    "june_18_mean_nin_2df.to_csv('june_18_mean_N_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9886"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(june_18_nin_df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9900"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(june_18_nin_2df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
